{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOofOuBQHL9r"
   },
   "source": [
    "# Deep Reinforcement Learning --- Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLe4U_TabApv"
   },
   "source": [
    "# The environment\n",
    "The environment was defined in the Maze class.<br><br>\n",
    "## The most important elements:\n",
    "### State\n",
    "State is returned by methods step(action) and state().<br>\n",
    "State consists of 5 values:\n",
    "* position, lastAction, reward, done, info*:\n",
    "* position --- tuple of two real numbers (y, x),\n",
    "* lastAction --- last action passed to step(),\n",
    "* reward --- real number,\n",
    "* done --- boolean value: True if game is over,\n",
    "\n",
    "### Info\n",
    "Method *info()* returns a dictionary containing information about the environment,\n",
    " * dimensions --- shape of the environment,\n",
    " * epsilon --- the probability of taking a random step,\n",
    " * actions --- available actions.\n",
    "\n",
    "### Action\n",
    "Action is determined by value: 1, 2, 4 or 8:\n",
    "*   1 --- right,\n",
    "*   2 --- left,\n",
    "*   4 --- down,\n",
    "*   8 --- up.\n",
    "\n",
    "### Reset\n",
    "reset(fixedPosition=True) resets the eivironment.<br>\n",
    "fixedPosition:\n",
    "* True --- agent starts at position 1, 1\n",
    "* False --- agent starts at random position\n",
    "\n",
    "### Visualisation\n",
    "* display() --- display image of actual environment state\n",
    "* showReplay() --- display video of the last game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62C4emZIH30a"
   },
   "source": [
    "# Loading environment\n",
    "**Put your ID in the *student_id* variable**\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wCVcc29UG86g",
    "ExecuteTime": {
     "end_time": "2024-11-30T12:57:11.306213Z",
     "start_time": "2024-11-30T12:57:11.286334Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Video, display\n",
    "import os\n",
    "\n",
    "class Cell:\n",
    "  cellType = 0\n",
    "  def __init__(self, cellType=0, reward=0, available=True, terminal=False):\n",
    "    self.reward = reward\n",
    "    self.walls = 0\n",
    "    self.cellType = cellType\n",
    "    self.available = available\n",
    "    self.terminal = terminal\n",
    "\n",
    "  def __str__(self):\n",
    "    return self.cellType\n",
    "\n",
    "class Sprite:\n",
    "  def __init__(self, img, mask):\n",
    "    self.img = img\n",
    "    self.mask = mask\n",
    "    self.width, self.height, _ = img.shape\n",
    "\n",
    "  def draw(self, img, position):\n",
    "    img[position[1] : position[1]+self.height, position[0] : position[0]+self.height] &= self.mask\n",
    "    img[position[1] : position[1]+self.height, position[0] : position[0]+self.height] |= self.img\n",
    "\n",
    "class Maze:\n",
    "  def __init__(self, maze, colors, player, background, resolution, epsilon=0):\n",
    "    self.maze = maze\n",
    "    self.colors = colors\n",
    "    self.player = player\n",
    "    self.resolution = resolution\n",
    "    self.epsilon=epsilon\n",
    "    self.background = background\n",
    "    self.mazeHeight, self.mazeWidth = maze.shape\n",
    "    self.imgWidth = self.mazeWidth * self.resolution\n",
    "    self.imgHeight = self.mazeHeight * self.resolution\n",
    "    self.reset()\n",
    "\n",
    "  def reset(self, fixedPosition=True):\n",
    "    self.reward = 0.0\n",
    "    self.cumulativeReward = 0.0\n",
    "    self.steps = 0\n",
    "    self.x = 1*fixedPosition\n",
    "    self.y = 1*fixedPosition\n",
    "    while(not self.maze[self.y, self.x].available):\n",
    "      self.x = np.random.randint(1, self.mazeWidth)\n",
    "      self.y = np.random.randint(1, self.mazeHeight)\n",
    "    self.replay_path = []\n",
    "    self.replay_path.append((self.y, self.x))\n",
    "\n",
    "  def info(self):\n",
    "    info = dict()\n",
    "    info[\"dimensions\"] = self.maze.shape\n",
    "    info[\"epsilon\"] = self.epsilon\n",
    "    info[\"actions\"] = (1, 2, 4, 8)\n",
    "    return info\n",
    "\n",
    "  def render(self):\n",
    "    border = 10\n",
    "    img = self.background.copy()\n",
    "    self.player.draw(img, (self.x*self.resolution, self.y*self.resolution))\n",
    "    img = img[self.resolution-border: -self.resolution+border, self.resolution-border: -self.resolution+border]\n",
    "    return img\n",
    "\n",
    "  def display(self):\n",
    "    plt.imshow(self.render())\n",
    "    plt.show()\n",
    "\n",
    "  def showReplay(self):\n",
    "    video_path = \"tmp.mp4\"\n",
    "    video = cv2.VideoWriter(video_path, cv2.VideoWriter_fourcc(*\"mp4v\"), 10.0, (env.render().shape[1::-1]))\n",
    "    for pos in self.replay_path:\n",
    "      self.y, self.x = pos\n",
    "      video.write(env.render())\n",
    "    video.release()\n",
    "    fileToDisplay = \"video.mp4\"\n",
    "    os.system(f\"ffmpeg -loglevel quiet -y -i {video_path} -vcodec libx264 -x264opts keyint=123:min-keyint=120 -an {fileToDisplay}\")\n",
    "    display(Video(fileToDisplay, embed=True))\n",
    "\n",
    "  def action(self, dir):\n",
    "    dir_e = int((np.random.randint(1,3)<<2)/((dir|dir>>1)&5)*(np.random.random()<self.epsilon))\n",
    "    dir = dir*(not dir_e) | dir_e\n",
    "    x=self.x + (dir&1) - (dir>>1&1)\n",
    "    y=self.y + (dir>>2&1) - (dir>>3&1)\n",
    "    self.x=x*self.maze[y, x].available + self.x*(not self.maze[y, x].available)\n",
    "    self.y=y*self.maze[y, x].available + self.y*(not self.maze[y, x].available)\n",
    "    self.replay_path.append((self.y, self.x))\n",
    "    return dir\n",
    "\n",
    "  def state(self, lastAction = 0):\n",
    "    done = self.maze[self.y, self.x].terminal\n",
    "    reward = self.maze[self.y, self.x].reward\n",
    "    info = self.info()\n",
    "    return (self.y, self.x), lastAction, reward, done\n",
    "\n",
    "  def step(self, action):\n",
    "    action = self.action(action)\n",
    "    return self.state(action)\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "b9C8gGvEH4z2",
    "ExecuteTime": {
     "end_time": "2024-11-30T12:57:11.338052Z",
     "start_time": "2024-11-30T12:57:11.333149Z"
    }
   },
   "source": [
    "from urllib.request import urlopen\n",
    "import pickle\n",
    "\n",
    "def download_dataset(filename):\n",
    "  base_url =f\"https://github.com/pa-k/DRL2022/blob/main/DRL_assignment1/{filename}?raw=true\"\n",
    "  url = urlopen(base_url)\n",
    "  binary_data = url.read()\n",
    "  with open(filename, \"wb\") as f:\n",
    "    f.write(binary_data)\n",
    "\n",
    "def loadEnv(student_id):\n",
    "  file_id = student_id%5\n",
    "  filename = f\"DRL_a{file_id}.pickle\"\n",
    "  if not os.path.exists(filename):\n",
    "    download_dataset(filename)\n",
    "  with open(filename, 'rb') as file2:\n",
    "    env = pickle.load(file2)\n",
    "  return env\n",
    "\n",
    "student_id = 0    #Your id\n",
    "env = loadEnv(student_id)\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1vnf1_2H6oy"
   },
   "source": [
    "#Printing environment and the state details"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZIBs_RLdII1P",
    "outputId": "ee0da900-e112-421c-985a-cc263dbfff39",
    "ExecuteTime": {
     "end_time": "2024-11-30T12:57:11.457705Z",
     "start_time": "2024-11-30T12:57:11.453876Z"
    }
   },
   "source": [
    "info = env.info()\n",
    "print(\"info\", info)\n",
    "print(\"environment dimensions\", info[\"dimensions\"])\n",
    "print(\"probability of random step\", info[\"epsilon\"])\n",
    "print(\"allowed actions\", info[\"actions\"])\n",
    "\n",
    "actPos, action, reward, done = env.state()\n",
    "print(\"\\nagent position\", actPos)\n",
    "print(\"performed action\", action)\n",
    "print(\"last reward\", reward)\n",
    "print(\"the game is over\", done)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info {'dimensions': (7, 8), 'epsilon': 0.0, 'actions': (1, 2, 4, 8)}\n",
      "environment dimensions (7, 8)\n",
      "probability of random step 0.0\n",
      "allowed actions (1, 2, 4, 8)\n",
      "\n",
      "agent position (1, 1)\n",
      "performed action 0\n",
      "last reward 0\n",
      "the game is over False\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuV7wkoJIK7J"
   },
   "source": [
    "#Visualization examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3OKuByhIcwd"
   },
   "source": [
    "##Example 1:\n",
    "###Print environment status and display visualization in four steps: right, right, left, down"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "PqBPDcYaIc8d",
    "outputId": "e10560c5-a257-4f6c-978a-0ab4eeec1220",
    "ExecuteTime": {
     "end_time": "2024-11-30T12:57:11.586935Z",
     "start_time": "2024-11-30T12:57:11.569641Z"
    }
   },
   "source": [
    "steps = [1, 1, 2, 4] # right, right, left, down\n",
    "env.reset()\n",
    "print(env.state())\n",
    "# env.display()\n",
    "for step in steps:\n",
    "  state = env.step(step)\n",
    "  print(\"\\n\\n\", state)\n",
    "  # env.display()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1, 1), 0, 0, False)\n",
      "\n",
      "\n",
      " ((1, 2), 1, 0, False)\n",
      "\n",
      "\n",
      " ((1, 3), 1, 0, False)\n",
      "\n",
      "\n",
      " ((1, 2), 2, 0, False)\n",
      "\n",
      "\n",
      " ((2, 2), 4, 0, False)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JO_MhqJWI5Gk",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##Example 2:\n",
    "### Generating a video of last game"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "id": "8jDnoN-OIdCe",
    "outputId": "f3fefe3b-3bd0-4eea-b5cc-86a7ec612b7f",
    "ExecuteTime": {
     "end_time": "2024-11-30T12:57:11.637663Z",
     "start_time": "2024-11-30T12:57:11.630144Z"
    }
   },
   "source": [
    "  env.reset()\n",
    "  env.step(1) # right\n",
    "  env.step(4) # down\n",
    "  env.step(2) # left\n",
    "  env.step(8) # up\n",
    "  env.step(1) # right\n",
    "  env.step(1) # right\n",
    "  env.step(1) # right\n",
    "  env.step(1) # right\n",
    "  env.step(1) # right\n",
    "  # env.showReplay()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 6), 1, 0, False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9SM11SlOJKFv"
   },
   "source": [
    "## Example 3:\n",
    "### Simple random agent with Visualization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "XcaLZtXoIdFw",
    "outputId": "ae6d49be-1051-4a2e-825f-79cc1ad9f011",
    "ExecuteTime": {
     "end_time": "2024-11-30T12:57:11.698176Z",
     "start_time": "2024-11-30T12:57:11.686611Z"
    }
   },
   "source": [
    "class RandomAgent:\n",
    "  def __init__(self, actions):\n",
    "    self.actions = actions\n",
    "\n",
    "  def play(self, state):\n",
    "    return np.random.choice(self.actions)\n",
    "\n",
    "env.reset()\n",
    "info = env.info()\n",
    "state = env.state()\n",
    "agent = RandomAgent(info[\"actions\"])\n",
    "for step in range(300):\n",
    "  action = agent.play(state)\n",
    "  state = env.step(action)\n",
    "  if state[3] == True:\n",
    "    break\n",
    "# env.showReplay()\n",
    "if(state[2]>0):\n",
    "  print(\"Yupi :)\")\n",
    "if(state[2]<0):\n",
    "  print(\":<\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":<\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1GvDOPPHbpx"
   },
   "source": [
    "# PL\n",
    "### Zadanie 1 --- max 20 p.\n",
    "\n",
    "Zaimplementuj algorytm Monte-Carlo do uczenia wartość *V*.<br>\n",
    "Kluczowe elementy:\n",
    "* Implementacja tablicy wartości *V*.\n",
    "* Opracowanie funkcji wyboru najlepszej akcji na podstawie tablicy wartości *V*.\n",
    "* Implementacja algorytmu aktualizacji tablicy wartości *V* na podstawie zebranych epizodów.\n",
    "* Implementacja dwóch metody eksploracji: e-greedy + inna metoda.\n",
    "* Dobór długości epizodów, liczby epizodów i epok.\n",
    "\n",
    "Przetestuj uczenie:\n",
    "* bez kary za krok,\n",
    "* z karą -0.1 za krok,\n",
    "* zaproponuj własną karę za krok i inne usprawnienia.\n",
    "\n",
    "Jak kara za krok wpływa na zaimplementowaną motodę uczenia wartości *V*?<br>\n",
    "Przedstaw wyniki oraz wnioski z uczenia wartości *V* metodą Monte-Carlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rycDXUL9HgyR"
   },
   "source": [
    "# Eng\n",
    "### Assignment 1 --- max 20 p.\n",
    "Implement the Monte Carlo algorithm for learning the *V-value* function.<br>\n",
    "Key elements:\n",
    "* V-value array implementation.\n",
    "* Development of a function to choose the best action based on the V-value array.\n",
    "* Implementation of an algorithm to update the *V-values* based on the collected episodes.\n",
    "* Implementation of the two exploration methods: e-greedy + other method.\n",
    "* Selection of the maximum length of episodes, number of episodes and epochs.\n",
    "\n",
    "Test learning:\n",
    "* without step penalty,\n",
    "* with -0.1 penalty per step,\n",
    "* suggest your own step penalty and other improvements.\n",
    "\n",
    "How does the step penalty affect the implemented V learning method? <br>\n",
    "Present the results and conclusions from learning the V-values using the Monte-Carlo method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2u14Ob9HmO_"
   },
   "source": "# My Solution"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AJF7BUHEHbWo",
    "ExecuteTime": {
     "end_time": "2024-11-30T12:57:11.749748Z",
     "start_time": "2024-11-30T12:57:11.747571Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
